{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RR-ljubciGY4"
   },
   "source": [
    "![alt text](https://i.imgur.com/1WaY7aA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nl1sQTCZiNhB"
   },
   "source": [
    "# Black Cat Data - Data Science and AI\n",
    "## Lab 10.5: Predict Text Using Keras\n",
    "\n",
    "We'll build a model which can predict the next word based on previous words.\n",
    "\n",
    "### What is Predictive Text?\n",
    "\n",
    "Predictive text is input technology where one key or button throws possibilities of many letters — such as on the numeric keypads of mobile phones and in accessibility technologies. Each key press results in a prediction rather than repeatedly sequencing through the same group of letters it represents, in the same, invariable order. [1](https://analyticsindiamag.com/predictive-text-analysis-changing-communication/)\n",
    "\n",
    "Here is a great example of predictive text where a bot have written a whole chapter of Harry Potter!\n",
    "\n",
    "**<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">We used predictive keyboards trained on all seven books to ghostwrite this spellbinding new Harry Potter chapter <a href=\"https://t.co/UaC6rMlqTy\">https://t.co/UaC6rMlqTy</a> <a href=\"https://t.co/VyxZwMYVVy\">pic.twitter.com/VyxZwMYVVy</a></p>&mdash; Botnik Studios (@botnikstudios) <a href=\"https://twitter.com/botnikstudios/status/940627812259696643?ref_src=twsrc%5Etfw\">December 12, 2017</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>**\n",
    "\n",
    "### Nietzsche\n",
    "\n",
    "We will use Friedrich Nietzsche’s [Beyond Good and Evil](https://en.wikipedia.org/wiki/Beyond_Good_and_Evil) as a training corpus for our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8p_NSkwjn-aD"
   },
   "source": [
    "## Load Corpus\n",
    "\n",
    "Load corpus and and use lowerscase to make all texts lower.\n",
    "\n",
    "What is the length of corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5vGU9nxZPMwa",
    "outputId": "d30930fa-3ddb-4aea-83c0-35b5eb8d2561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600901\n"
     ]
    }
   ],
   "source": [
    "path = '../DATA/nietzsche.txt'\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNq9fntvocRX"
   },
   "source": [
    "## Load Necessary Libraries\n",
    "\n",
    "We'll be using tenserflow, keras for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJZCMG8NQ8gJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "import heapq\n",
    "import seaborn as sns\n",
    "\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PD4txnFForIo"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Unique Characters of Corpus\n",
    "\n",
    "Let's find out all unique characters in the corpus . Create char to index and index to char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K8LhasL3Q87U",
    "outputId": "e5a8b734-74f2-4dce-972d-35b149655b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique chars: 59\n"
     ]
    }
   ],
   "source": [
    "# Find out all unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# Create char to index and index to char.\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print(f'unique chars: {len(chars)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZP4UHFdpd_q"
   },
   "source": [
    "## Create Chunk and Next Characters for Each Chunk\n",
    "\n",
    "Next, let’s cut the corpus into chunks of 40 characters, spacing the sequences by 3 characters. Additionally, we will store the next character (the one we need to predict) for every sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D9Aspm6eQ_Xg",
    "outputId": "d65709cf-e03e-4201-8d7d-a663d736fcbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num training examples: 200287\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - SEQUENCE_LENGTH, step):\n",
    "    sentences.append(text[i: i + SEQUENCE_LENGTH])\n",
    "    next_chars.append(text[i + SEQUENCE_LENGTH])\n",
    "print(f'num training examples: {len(sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_-_Tru5p4Fq"
   },
   "source": [
    "## Features & Target Variables\n",
    "\n",
    "It is time for generating our features and labels. We will use the previously generated sequences and characters that need to be predicted to create one-hot encoded vectors using the char_indices map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y21zq-xiRJPC"
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(sentences), SEQUENCE_LENGTH, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I8B-ZbDbqh6D"
   },
   "source": [
    "Let’s have a look at a single training sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xScz1B6aqZYR",
    "outputId": "f2a44d6b-94c3-4f7b-b9c3-6fa8798e234c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ve been unskilled and unseemly methods f'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Irxkd-wqoQO"
   },
   "source": [
    "The character that needs to be predicted for it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-bYSWMYxqqqD",
    "outputId": "d62f7732-c9a9-481a-923b-0666a41d9d47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_chars[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a_BOjsnlrIyu",
    "outputId": "80500809-b843-4c86-ad1e-593998ce55c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200287, 40, 59)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FP8w9AjPqxL6"
   },
   "source": [
    "The encoded (one-hot) data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "xKyAvYSNqvA6",
    "outputId": "74f8f580-0303-484c-f07e-7bdab48b318f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "xwzRu0mMrESL",
    "outputId": "d7c2494b-b5f7-49e3-f2e8-285a91e38b01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gyVfLdN1rw8S"
   },
   "source": [
    "## Building Model\n",
    "\n",
    "The model we’re going to train is pretty straight forward. Single LSTM layer with 128 neurons which accepts input of shape (40 — the length of a sequence, 57 — the number of unique characters in our dataset). A fully connected layer (for our output) is added after that. It has 57 neurons and softmax for activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "ou-U1XXJRWOw",
    "outputId": "cd490a97-6b6c-4580-e588-5cbc2f900f4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Beau\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(SEQUENCE_LENGTH, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qe3xo2eUr6uJ"
   },
   "source": [
    "## Training \n",
    "\n",
    "Our model is trained for 20 epochs using RMSProp optimizer and uses 5% of the data for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "colab_type": "code",
    "id": "iz871ihbRhRh",
    "outputId": "9dacff9f-d039-43a8-c9e5-919150021f39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Beau\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 190272 samples, validate on 10015 samples\n",
      "Epoch 1/20\n",
      " 29696/190272 [===>..........................] - ETA: 3:28 - loss: 2.5946 - acc: 0.2678"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(X, y, validation_split=0.05, batch_size=128, epochs=20, shuffle=True).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EcPWqWe3r_-N"
   },
   "source": [
    "## Save and Load Model\n",
    "\n",
    "It's time consuming to train a model. We'll save our trained model and load it to avoid trainng time later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T09:13:58.827604Z",
     "start_time": "2019-07-22T09:13:58.823617Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0kEXSqMXRjxM"
   },
   "outputs": [],
   "source": [
    "model.save('keras_model.h5')\n",
    "pickle.dump(history, open(\"history.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T09:14:12.996256Z",
     "start_time": "2019-07-22T09:14:12.994258Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "wIdK4aPMhF6Z"
   },
   "outputs": [],
   "source": [
    "model = load_model('keras_model.h5')\n",
    "history = pickle.load(open(\"history.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBqHejNksVFo"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Let’s have a look at how our accuracy and loss change over training epochs:\n",
    "\n",
    "### Evaluate Accuracy\n",
    "\n",
    "Create a plot of \n",
    "    \n",
    "    history['acc']\n",
    "    history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "7rpPShnnhH58",
    "outputId": "266c8639-a4db-46f2-9f69-f2d1591a180c"
   },
   "outputs": [],
   "source": [
    "plt.plot(history['acc'])\n",
    "plt.plot(history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0QJD6klsXj4"
   },
   "source": [
    "### Evaluate Loss\n",
    "\n",
    "Create a plot of\n",
    "\n",
    "    history['loss']\n",
    "    history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "Ueh-njPkhLBi",
    "outputId": "31283db5-39da-4dda-95ee-f7dc1634bd89"
   },
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JvWiaTRvsfrw"
   },
   "source": [
    "## Test\n",
    "\n",
    "Finally, it is time to predict some word completions using our model! First, we need some helper functions. Let’s start by preparing our input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6IFpQHLhOFj"
   },
   "outputs": [],
   "source": [
    "def prepare_input(text):\n",
    "    x = np.zeros((1, SEQUENCE_LENGTH, len(chars)))\n",
    "    for t, char in enumerate(text):\n",
    "        x[0, t, char_indices[char]] = 1.\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_6QQDmbszOc"
   },
   "source": [
    "Remember that our sequences must be 40 characters long. So we make a tensor with shape (1, 40, 57), initialized with zeros. Then, a value of 1 is placed for each character in the passed text. We must not forget to use the lowercase version of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "eif1xnwThQTc",
    "outputId": "6b2a6fc7-4e27-4a03-e8f0-c9e3e9f9f439"
   },
   "outputs": [],
   "source": [
    "prepare_input(\"This is an example of input for our LSTM\".lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QzyaG3G_s6vf"
   },
   "source": [
    "Next up, the sample function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JsvvbgJ-hSRH"
   },
   "outputs": [],
   "source": [
    "def sample(preds, top_n=3):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    return heapq.nlargest(top_n, range(len(preds)), preds.take)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VjGOkGn5s9Gf"
   },
   "source": [
    "This function allows us to ask our model what are the next n most probable characters.\n",
    "\n",
    "\n",
    "Now for the prediction functions themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6yI3_eJShUFR"
   },
   "outputs": [],
   "source": [
    "def predict_completion(text):\n",
    "    original_text = text\n",
    "    generated = text\n",
    "    completion = ''\n",
    "    while True:\n",
    "        x = prepare_input(text)\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, top_n=1)[0]\n",
    "        next_char = indices_char[next_index]\n",
    "        text = text[1:] + next_char\n",
    "        completion += next_char\n",
    "        \n",
    "        if len(original_text + completion) + 2 > len(original_text) and \\\n",
    "            (next_char == ' ' or next_char == ',' or next_char == ';'):\n",
    "            return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7-noC6FtIkS"
   },
   "source": [
    "This function predicts next character until space is predicted (you can extend that to punctuation symbols, right?). It does so by repeatedly preparing input, asking our model for predictions and sampling from them.\n",
    "\n",
    "The final piece of the puzzle — `predict_completions` wraps everything and allow us to predict multiple completions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ogbe_oNehWQz"
   },
   "outputs": [],
   "source": [
    "def predict_completions(text, n=3):\n",
    "    x = prepare_input(text)\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_indices = sample(preds, n)\n",
    "    return [indices_char[idx] + predict_completion(text[1:] + indices_char[idx]) for idx in next_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFtAbg0CtPj4"
   },
   "source": [
    "Let’s use sequences of 40 characters that we will use as seed for our completions. All of these are quotes from Friedrich Nietzsche himself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i4vHEN94hYgn"
   },
   "outputs": [],
   "source": [
    "quotes = [\n",
    "    \"It is not a lack of love, but a lack of friendship that makes unhappy marriages.\",\n",
    "    \"That which does not kill us makes us stronger.\",\n",
    "    \"I'm not upset that you lied to me, I'm upset that from now on I can't believe you.\",\n",
    "    \"And those who were seen dancing were thought to be insane by those who could not hear the music.\",\n",
    "    \"It is hard enough to remember my opinions, without also remembering my reasons for them!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "z_n06iJzhaHY",
    "outputId": "f4c0967e-92b1-443e-cf19-ae83b2662e71"
   },
   "outputs": [],
   "source": [
    "for q in quotes:\n",
    "    seq = q[:40].lower()\n",
    "    print(seq)\n",
    "    print(predict_completions(seq, 5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILjYvmEttWYr"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We’ve built a model using just a few lines of code in Keras that performs reasonably well after just 20 training epochs. Can you try it with another corpus? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUS8zEuqtb7u"
   },
   "source": [
    "## References\n",
    "\n",
    "[Recurrent Nets in TensorFlow](https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/?source=post_page---------------------------)\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/?source=post_page---------------------------)\n",
    "\n",
    "[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/?source=post_page---------------------------)\n",
    "\n",
    "[How to implement RNN in Python](https://peterroelants.github.io/posts/rnn_implementation_part01/?source=post_page---------------------------)\n",
    "\n",
    "[LSTM Networks for Sentiment Analysis](http://deeplearning.net/tutorial/lstm.html?source=post_page---------------------------)\n",
    "\n",
    "[cs231n — Recurrent Neural Networks](http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf?source=post_page---------------------------)\n",
    "\n",
    "[Making a Predictive Keyboard using Recurrent Neural Networks](https://medium.com/@curiousily/making-a-predictive-keyboard-using-recurrent-neural-networks-tensorflow-for-hackers-part-v-3f238d824218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSIA Lab 10.4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
