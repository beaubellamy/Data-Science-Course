{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DSIA Demo-9_7.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["_Pck1cuvLtDH","mQCAUFWYLtDb","OXwLriDpLtDq","-2oNfajULtD4","q1wYto68LtEE","gLGxWK0yLtEO"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"Nji1a9ULLtCA","colab_type":"text"},"cell_type":"markdown","source":["![image.png](https://i.imgur.com/1WaY7aA.png)\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"metadata":{"id":"fnsX1AWKLtCE","colab_type":"text"},"cell_type":"markdown","source":["# Data Science and AI\n","## Demo 9.7: Text Classification\n","INSTRUCTIONS:\n","- Run the cells\n","- Observe and understand the results\n","- Answer the questions"]},{"metadata":{"id":"6pm8PttyLtCI","colab_type":"text"},"cell_type":"markdown","source":["## Import libraries"]},{"metadata":{"id":"EUANiH6zLtCK","colab_type":"code","colab":{}},"cell_type":"code","source":["## Import Libraries\n","import numpy as np\n","import pandas as pd\n","\n","import string\n","import spacy\n","\n","from collections import Counter\n","\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC\n","\n","# import warnings\n","# warnings.filterwarnings('ignore')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"58bUNQA0LtCV","colab_type":"text"},"cell_type":"markdown","source":["## Load data"]},{"metadata":{"id":"UqU7d_qcLtCX","colab_type":"text"},"cell_type":"markdown","source":["Sample:\n","\n","    __label__2 Stuning even for the non-gamer: This sound ...\n","    __label__2 The best soundtrack ever to anything.: I'm ...\n","    __label__2 Amazing!: This soundtrack is my favorite m ...\n","    __label__2 Excellent Soundtrack: I truly like this so ...\n","    __label__2 Remember, Pull Your Jaw Off The Floor Afte ...\n","    __label__2 an absolute masterpiece: I am quite sure a ...\n","    __label__1 Buyer beware: This is a self-published boo ...\n","    . . .\n","    \n","There are only two **labels**:\n","- `__label__1`\n","- `__label__2`"]},{"metadata":{"id":"rwWFJprZLtCZ","colab_type":"code","colab":{}},"cell_type":"code","source":["## Loading the data\n","\n","trainDF = pd.read_fwf(\n","    filepath_or_buffer = './data/corpus.txt',\n","    colspecs = [(9, 10),   # label: get only the numbers 1 or 2\n","                (11, 9000) # text: makes the it big enought to get to the end of the line\n","               ], \n","    header = 0,\n","    names = ['label', 'text'],\n","    lineterminator = '\\n'\n",")\n","\n","# convert label from [1, 2] to [0, 1]\n","trainDF['label'] = trainDF['label'] - 1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mILVIHomLtCf","colab_type":"text"},"cell_type":"markdown","source":["## Inspect the data"]},{"metadata":{"id":"G9_8RbOeLtCh","colab_type":"code","colab":{},"outputId":"03171e73-f977-4da3-bc96-3b446b69497c"},"cell_type":"code","source":["print(trainDF.info())\n","print(trainDF.sample(10))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 9999 entries, 0 to 9998\n","Data columns (total 2 columns):\n","label    9999 non-null int64\n","text     9999 non-null object\n","dtypes: int64(1), object(1)\n","memory usage: 156.3+ KB\n","None\n","      label                                               text\n","4771      0  Many Flaws in Mellor's 'Nursery Knits': This b...\n","734       1  Excellent Book: I read this book for my 11th g...\n","5746      1  Armed Response: is essential: If your looking ...\n","472       0  A Piece of Crap!: Noisy, crude five point vibr...\n","7228      1  This book changed the way I look at all books....\n","9898      0  DUMB!: This movie is a bunch of nonsense. A gr...\n","8795      1  You gotta love garbage: After missing theirr r...\n","3859      1  A complex, multi-layered masterpiece: Unlike D...\n","1316      1  Everyone should read this book.: I first heard...\n","4478      1  Invaluable: The chapters on \"Genuine Encounter...\n"],"name":"stdout"}]},{"metadata":{"id":"6YmYgG2pLtCu","colab_type":"text"},"cell_type":"markdown","source":["## Split the data into train and test"]},{"metadata":{"id":"j5vErjWFLtCy","colab_type":"code","colab":{}},"cell_type":"code","source":["## split the dataset\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    trainDF['text'],\n","    trainDF['label'], \n","    test_size = 0.2,\n","    random_state = 42\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6nUp6oDOLtC1","colab_type":"text"},"cell_type":"markdown","source":["## Feature Engineering"]},{"metadata":{"id":"fKd9yTnyLtC2","colab_type":"text"},"cell_type":"markdown","source":["### Count Vectors as features"]},{"metadata":{"id":"DU2RqqDjLtC3","colab_type":"code","colab":{}},"cell_type":"code","source":["# create a count vectorizer object\n","count_vect = CountVectorizer(token_pattern = r'\\w{1,}')\n","\n","# Learn a vocabulary dictionary of all tokens in the raw documents\n","count_vect.fit(trainDF['text'])\n","\n","# Transform documents to document-term matrix.\n","X_train_count = count_vect.transform(X_train)\n","X_test_count = count_vect.transform(X_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dJs6al0ILtC5","colab_type":"text"},"cell_type":"markdown","source":["### TF-IDF Vectors as features\n","- Word level\n","- N-Gram level\n","- Character level"]},{"metadata":{"id":"myjfdfP_LtC6","colab_type":"code","colab":{},"outputId":"8bc4d529-1f66-4836-acd1-07633c29fd02"},"cell_type":"code","source":["%%time\n","# word level tf-idf\n","tfidf_vect = TfidfVectorizer(analyzer = 'word',\n","                             token_pattern = r'\\w{1,}',\n","                             max_features = 5000)\n","print(tfidf_vect)\n","\n","tfidf_vect.fit(trainDF['text'])\n","X_train_tfidf = tfidf_vect.transform(X_train)\n","X_test_tfidf  = tfidf_vect.transform(X_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n","        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n","        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n","        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n","        stop_words=None, strip_accents=None, sublinear_tf=False,\n","        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n","        vocabulary=None)\n","CPU times: user 1.3 s, sys: 26.1 ms, total: 1.32 s\n","Wall time: 1.32 s\n"],"name":"stdout"}]},{"metadata":{"id":"-h16dUaVLtC_","colab_type":"code","colab":{},"outputId":"3be1f8f5-670e-4249-8522-50509c8898a0"},"cell_type":"code","source":["%%time\n","# ngram level tf-idf\n","tfidf_vect_ngram = TfidfVectorizer(analyzer = 'word',\n","                                   token_pattern = r'\\w{1,}',\n","                                   ngram_range = (2, 3),\n","                                   max_features = 5000)\n","print(tfidf_vect_ngram)\n","\n","tfidf_vect_ngram.fit(trainDF['text'])\n","X_train_tfidf_ngram = tfidf_vect_ngram.transform(X_train)\n","X_test_tfidf_ngram  = tfidf_vect_ngram.transform(X_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n","        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n","        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n","        ngram_range=(2, 3), norm='l2', preprocessor=None, smooth_idf=True,\n","        stop_words=None, strip_accents=None, sublinear_tf=False,\n","        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n","        vocabulary=None)\n","CPU times: user 6.9 s, sys: 229 ms, total: 7.13 s\n","Wall time: 6.55 s\n"],"name":"stdout"}]},{"metadata":{"id":"Y7rmIt49LtDC","colab_type":"code","colab":{},"outputId":"8f600e7c-b4df-4d89-bfb9-96c8436aa5ef"},"cell_type":"code","source":["%%time\n","# characters level tf-idf\n","tfidf_vect_ngram_chars = TfidfVectorizer(analyzer = 'char',\n","                                         token_pattern = r'\\w{1,}',\n","                                         ngram_range = (2, 3),\n","                                         max_features = 5000)\n","print(tfidf_vect_ngram_chars)\n","\n","tfidf_vect_ngram_chars.fit(trainDF['text'])\n","X_train_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_train)\n","X_test_tfidf_ngram_chars  = tfidf_vect_ngram_chars.transform(X_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n","        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n","        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n","        ngram_range=(2, 3), norm='l2', preprocessor=None, smooth_idf=True,\n","        stop_words=None, strip_accents=None, sublinear_tf=False,\n","        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n","        vocabulary=None)\n","CPU times: user 9.63 s, sys: 205 ms, total: 9.83 s\n","Wall time: 9.84 s\n"],"name":"stdout"}]},{"metadata":{"id":"_Pck1cuvLtDH","colab_type":"text"},"cell_type":"markdown","source":["### Text / NLP based features"]},{"metadata":{"id":"4jebGm1gLtDH","colab_type":"code","colab":{},"outputId":"124d1ac4-9b64-414d-e973-9ded78662e18"},"cell_type":"code","source":["%%time\n","trainDF['char_count'] = trainDF['text'].apply(len)\n","trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n","trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count'] + 1)\n","trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(''.join(_ for _ in x if _ in string.punctuation))) \n","trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([w for w in x.split() if w.istitle()]))\n","trainDF['uppercase_word_count'] = trainDF['text'].apply(lambda x: len([w for w in x.split() if w.isupper()]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 665 ms, sys: 2.84 ms, total: 667 ms\n","Wall time: 667 ms\n"],"name":"stdout"}]},{"metadata":{"id":"SZ-wnhIvLtDL","colab_type":"code","colab":{},"outputId":"69d3e28a-291d-4111-9cbc-cc21c37b9037"},"cell_type":"code","source":["trainDF.sample(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","      <th>char_count</th>\n","      <th>word_count</th>\n","      <th>word_density</th>\n","      <th>punctuation_count</th>\n","      <th>title_word_count</th>\n","      <th>uppercase_word_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>558</th>\n","      <td>0</td>\n","      <td>Wasn't what I expected: I ordered the item acc...</td>\n","      <td>266</td>\n","      <td>55</td>\n","      <td>4.750000</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>2969</th>\n","      <td>0</td>\n","      <td>It's time for human slaves to meet their maste...</td>\n","      <td>550</td>\n","      <td>101</td>\n","      <td>5.392157</td>\n","      <td>27</td>\n","      <td>10</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1405</th>\n","      <td>1</td>\n","      <td>Perfect gift for the family: Purchased iwth Am...</td>\n","      <td>213</td>\n","      <td>39</td>\n","      <td>5.325000</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4540</th>\n","      <td>1</td>\n","      <td>thought provoking: I read this book for the se...</td>\n","      <td>457</td>\n","      <td>83</td>\n","      <td>5.440476</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>6291</th>\n","      <td>1</td>\n","      <td>Nice for NYG Fans!: Very nice Floor Mats. Boug...</td>\n","      <td>262</td>\n","      <td>49</td>\n","      <td>5.240000</td>\n","      <td>8</td>\n","      <td>13</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      label                                               text  char_count  \\\n","558       0  Wasn't what I expected: I ordered the item acc...         266   \n","2969      0  It's time for human slaves to meet their maste...         550   \n","1405      1  Perfect gift for the family: Purchased iwth Am...         213   \n","4540      1  thought provoking: I read this book for the se...         457   \n","6291      1  Nice for NYG Fans!: Very nice Floor Mats. Boug...         262   \n","\n","      word_count  word_density  punctuation_count  title_word_count  \\\n","558           55      4.750000                  6                 6   \n","2969         101      5.392157                 27                10   \n","1405          39      5.325000                  4                 6   \n","4540          83      5.440476                  9                 9   \n","6291          49      5.240000                  8                13   \n","\n","      uppercase_word_count  \n","558                      5  \n","2969                     2  \n","1405                     0  \n","4540                     4  \n","6291                     3  "]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"scrolled":false,"id":"Z-l2iZcLLtDO","colab_type":"code","colab":{}},"cell_type":"code","source":["## load spaCy\n","nlp = spacy.load('en')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p-9d0G59LtDR","colab_type":"text"},"cell_type":"markdown","source":["Part of Speech in **SpaCy**\n","\n","    POS   DESCRIPTION               EXAMPLES\n","    ----- ------------------------- ---------------------------------------------\n","    ADJ   adjective                 big, old, green, incomprehensible, first\n","    ADP   adposition                in, to, during\n","    ADV   adverb                    very, tomorrow, down, where, there\n","    AUX   auxiliary                 is, has (done), will (do), should (do)\n","    CONJ  conjunction               and, or, but\n","    CCONJ coordinating conjunction  and, or, but\n","    DET   determiner                a, an, the\n","    INTJ  interjection              psst, ouch, bravo, hello\n","    NOUN  noun                      girl, cat, tree, air, beauty\n","    NUM   numeral                   1, 2017, one, seventy-seven, IV, MMXIV\n","    PART  particle                  's, not,\n","    PRON  pronoun                   I, you, he, she, myself, themselves, somebody\n","    PROPN proper noun               Mary, John, London, NATO, HBO\n","    PUNCT punctuation               ., (, ), ?\n","    SCONJ subordinating conjunction if, while, that\n","    SYM   symbol                    $, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù\n","    VERB  verb                      run, runs, running, eat, ate, eating\n","    X     other                     sfpksdpsxmsa\n","    SPACE space"]},{"metadata":{"id":"NcxmvIOGLtDS","colab_type":"code","colab":{}},"cell_type":"code","source":["# Initialise some columns for feature's counts\n","trainDF['adj_count'] = 0\n","trainDF['adv_count'] = 0\n","trainDF['noun_count'] = 0\n","trainDF['num_count'] = 0\n","trainDF['pron_count'] = 0\n","trainDF['propn_count'] = 0\n","trainDF['verb_count'] = 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hxVRdplBLtDU","colab_type":"code","colab":{},"outputId":"3bea0c5a-3d73-4750-fcc7-9ece84e8cf7c"},"cell_type":"code","source":["%%time\n","# for each text\n","for i in range(trainDF.shape[0]):\n","    # convert into a spaCy document\n","    doc = nlp(trainDF.iloc[i]['text'])\n","    # initialise feature counters\n","    c = Counter([t.pos_ for t in doc])\n","\n","    trainDF.at[i, 'adj_count'] = c['ADJ']\n","    trainDF.at[i, 'adv_count'] = c['ADV']\n","    trainDF.at[i, 'noun_count'] = c['NOUN']\n","    trainDF.at[i, 'num_count'] = c['NUM']\n","    trainDF.at[i, 'pron_count'] = c['PRON']\n","    trainDF.at[i, 'propn_count'] = c['PROPN']\n","    trainDF.at[i, 'verb_count'] = c['VERB']"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 15min 33s, sys: 43 s, total: 16min 16s\n","Wall time: 4min 6s\n"],"name":"stdout"}]},{"metadata":{"scrolled":false,"id":"DW1_LKP2LtDX","colab_type":"code","colab":{},"outputId":"7a5eb5fd-cae1-4e76-f95f-e0fe9f1780d8"},"cell_type":"code","source":["cols = [\n","    'char_count', 'word_count', 'word_density',\n","    'punctuation_count', 'title_word_count',\n","    'uppercase_word_count', 'adj_count',\n","    'adv_count', 'noun_count', 'num_count',\n","    'pron_count', 'propn_count', 'verb_count']\n","\n","trainDF[cols].sample(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>char_count</th>\n","      <th>word_count</th>\n","      <th>word_density</th>\n","      <th>punctuation_count</th>\n","      <th>title_word_count</th>\n","      <th>uppercase_word_count</th>\n","      <th>adj_count</th>\n","      <th>adv_count</th>\n","      <th>noun_count</th>\n","      <th>num_count</th>\n","      <th>pron_count</th>\n","      <th>propn_count</th>\n","      <th>verb_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>8797</th>\n","      <td>213</td>\n","      <td>36</td>\n","      <td>5.756757</td>\n","      <td>8</td>\n","      <td>9</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>904</th>\n","      <td>407</td>\n","      <td>79</td>\n","      <td>5.087500</td>\n","      <td>16</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>11</td>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>6</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>6716</th>\n","      <td>285</td>\n","      <td>51</td>\n","      <td>5.480769</td>\n","      <td>9</td>\n","      <td>13</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>2727</th>\n","      <td>375</td>\n","      <td>64</td>\n","      <td>5.769231</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>15</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>5764</th>\n","      <td>555</td>\n","      <td>92</td>\n","      <td>5.967742</td>\n","      <td>17</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>7</td>\n","      <td>25</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>12</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      char_count  word_count  word_density  punctuation_count  \\\n","8797         213          36      5.756757                  8   \n","904          407          79      5.087500                 16   \n","6716         285          51      5.480769                  9   \n","2727         375          64      5.769231                  6   \n","5764         555          92      5.967742                 17   \n","\n","      title_word_count  uppercase_word_count  adj_count  adv_count  \\\n","8797                 9                     4          3          3   \n","904                 11                     2         11          4   \n","6716                13                     3          4          7   \n","2727                 4                     0          8          1   \n","5764                 9                     0          8          7   \n","\n","      noun_count  num_count  pron_count  propn_count  verb_count  \n","8797           4          1           6            4           8  \n","904           10          1           9            6          17  \n","6716           5          1           4            8           9  \n","2727          15          1           2            2           8  \n","5764          25          0           1            6          12  "]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"mQCAUFWYLtDb","colab_type":"text"},"cell_type":"markdown","source":["### Topic Models as features"]},{"metadata":{"id":"wg2mAlkRLtDb","colab_type":"code","colab":{},"outputId":"323bfce4-9263-403a-9214-e9e206d5755f"},"cell_type":"code","source":["%%time\n","# train a LDA Model\n","lda_model = LatentDirichletAllocation(n_components = 20, learning_method = 'online', max_iter = 20)\n","\n","X_topics = lda_model.fit_transform(X_train_count)\n","topic_word = lda_model.components_ \n","vocab = count_vect.get_feature_names()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 6min 41s, sys: 17.1 s, total: 6min 58s\n","Wall time: 1min 49s\n"],"name":"stdout"}]},{"metadata":{"id":"_8dIDyHjLtDf","colab_type":"code","colab":{},"outputId":"ea0614e2-66f1-4ddd-bff0-142cfd4fd78a"},"cell_type":"code","source":["# view the topic models\n","n_top_words = 10\n","topic_summaries = []\n","print('Group Top Words')\n","print('-----', '-'*80)\n","for i, topic_dist in enumerate(topic_word):\n","    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n","    top_words = ' '.join(topic_words)\n","    topic_summaries.append(top_words)\n","    print('  %3d %s' % (i, top_words))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Group Top Words\n","----- --------------------------------------------------------------------------------\n","    0 my product these they with use size them fit works\n","    1 richard displayed linksys pool blowing scent associated shoot ra buffalo\n","    2 the i and a to it of this is in\n","    3 1984 orwell print book printer government brother economics hp george\n","    4 toy trash eat son shallow error stephen rabbit extras recipes\n","    5 the cd is music you album game of this and\n","    6 fiction science 451 bradbury his foundation season fahrenheit novel future\n","    7 hatebreed circuit realy aluminum dancers reduce exuviance acne shades mentality\n","    8 arrived cable himself missing mystery situations include daily text god\n","    9 tape theory higgins team chapters tomcat hockey everest disney bound\n","   10 de la y en el que los del es un\n","   11 l spanish et le il pour titan dali est tism\n","   12 creepy analysis techniques hopkins tuscan cinema dummy equipment ourselves starring\n","   13 marquez emarker laughs jimmy garcia treasure campbell bums coast lighter\n","   14 christmas door per versions magazine stands seconds childhood blocks harry\n","   15 player camcorder mp3 lady cities advise superficial setup initial sequence\n","   16 cap pale backup identical blockbuster steer streaming unheard misfortune quit\n","   17 adapter apple mad g4 powerbook ibook macally diary ac light\n","   18 buddy paris torn colonel emma ry knox bernie neighbors rat\n","   19 rome jay compartment stolen yea theology squares creations abroad pal\n"],"name":"stdout"}]},{"metadata":{"id":"TtfnK1jeLtDl","colab_type":"text"},"cell_type":"markdown","source":["## Modelling"]},{"metadata":{"id":"uwVaWSyTLtDm","colab_type":"code","colab":{}},"cell_type":"code","source":["## helper function\n","\n","def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n","    # fit the training dataset on the classifier\n","    classifier.fit(feature_vector_train, label)\n","\n","    # predict the labels on validation dataset\n","    predictions = classifier.predict(feature_vector_valid)\n","\n","    return accuracy_score(predictions, y_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f_onpqUkLtDo","colab_type":"code","colab":{}},"cell_type":"code","source":["# Keep the results in a dataframe\n","results = pd.DataFrame(columns = ['Count Vectors',\n","                                  'WordLevel TF-IDF',\n","                                  'N-Gram Vectors',\n","                                  'CharLevel Vectors'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OXwLriDpLtDq","colab_type":"text"},"cell_type":"markdown","source":["### Naive Bayes Classifier"]},{"metadata":{"id":"ZcU6IKyNLtDs","colab_type":"code","colab":{},"outputId":"d2defcfb-2046-47e1-b3b6-08d6edca94f8"},"cell_type":"code","source":["%%time\n","# Naive Bayes on Count Vectors\n","accuracy1 = train_model(MultinomialNB(), X_train_count, y_train, X_test_count)\n","print('NB, Count Vectors    : %.4f\\n' % accuracy1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["NB, Count Vectors    : 0.8540\n","\n","CPU times: user 20.5 ms, sys: 3.79 ms, total: 24.3 ms\n","Wall time: 12.5 ms\n"],"name":"stdout"}]},{"metadata":{"id":"zqEG_ByTLtDv","colab_type":"code","colab":{},"outputId":"0ba74f49-a71c-4d59-f57b-f39f546241ce"},"cell_type":"code","source":["%%time\n","# Naive Bayes on Word Level TF IDF Vectors\n","accuracy2 = train_model(MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf)\n","print('NB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["NB, WordLevel TF-IDF : 0.8600\n","\n","CPU times: user 26.9 ms, sys: 4.35 ms, total: 31.2 ms\n","Wall time: 7.82 ms\n"],"name":"stdout"}]},{"metadata":{"id":"uKEEPNi8LtDy","colab_type":"code","colab":{},"outputId":"c00cf532-c914-4670-cde5-69bf54bf7201"},"cell_type":"code","source":["%%time\n","# Naive Bayes on Ngram Level TF IDF Vectors\n","accuracy3 = train_model(MultinomialNB(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n","print('NB, N-Gram Vectors   : %.4f\\n' % accuracy3)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["NB, N-Gram Vectors   : 0.8400\n","\n","CPU times: user 22 ms, sys: 3.97 ms, total: 26 ms\n","Wall time: 6.49 ms\n"],"name":"stdout"}]},{"metadata":{"id":"M9aIibkBLtD0","colab_type":"code","colab":{},"outputId":"1f52c1f4-150d-4195-f927-cb66ec41baba"},"cell_type":"code","source":["%%time\n","# # Naive Bayes on Character Level TF IDF Vectors\n","accuracy4 = train_model(MultinomialNB(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n","print('NB, CharLevel Vectors: %.4f\\n' % accuracy4)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["NB, CharLevel Vectors: 0.8180\n","\n","CPU times: user 118 ms, sys: 9.12 ms, total: 127 ms\n","Wall time: 32.1 ms\n"],"name":"stdout"}]},{"metadata":{"id":"kkrodUzCLtD3","colab_type":"code","colab":{}},"cell_type":"code","source":["results.loc['Na√Øve Bayes'] = {\n","    'Count Vectors': accuracy1,\n","    'WordLevel TF-IDF': accuracy2,\n","    'N-Gram Vectors': accuracy3,\n","    'CharLevel Vectors': accuracy4}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-2oNfajULtD4","colab_type":"text"},"cell_type":"markdown","source":["### Linear Classifier"]},{"metadata":{"id":"OFBhhPZ6LtD4","colab_type":"code","colab":{},"outputId":"cb5a2b19-dfc1-4b11-e698-0488ba4eae53"},"cell_type":"code","source":["%%time\n","# Linear Classifier on Count Vectors\n","accuracy1 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 350), X_train_count, y_train, X_test_count)\n","print('LR, Count Vectors    : %.4f\\n' % accuracy1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["LR, Count Vectors    : 0.8520\n","\n","CPU times: user 8.28 s, sys: 326 ms, total: 8.6 s\n","Wall time: 2.18 s\n"],"name":"stdout"}]},{"metadata":{"id":"C89hRhDiLtD6","colab_type":"code","colab":{},"outputId":"023669b1-788f-4a1b-d282-99464e26312c"},"cell_type":"code","source":["%%time\n","# Linear Classifier on Word Level TF IDF Vectors\n","accuracy2 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf, y_train, X_test_tfidf)\n","print('LR, WordLevel TF-IDF : %.4f\\n' % accuracy2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["LR, WordLevel TF-IDF : 0.8730\n","\n","CPU times: user 333 ms, sys: 15.5 ms, total: 348 ms\n","Wall time: 87.4 ms\n"],"name":"stdout"}]},{"metadata":{"id":"MvhV1jC6LtD9","colab_type":"code","colab":{},"outputId":"9bef9b1d-2dc3-4300-c4ff-831957aadee4"},"cell_type":"code","source":["%%time\n","# Linear Classifier on Ngram Level TF IDF Vectors\n","accuracy3 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n","print('LR, N-Gram Vectors   : %.4f\\n' % accuracy3)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["LR, N-Gram Vectors   : 0.8360\n","\n","CPU times: user 233 ms, sys: 13.2 ms, total: 246 ms\n","Wall time: 61.9 ms\n"],"name":"stdout"}]},{"metadata":{"id":"XPjIxmtKLtEA","colab_type":"code","colab":{},"outputId":"58f4b0e9-e786-45a1-830e-5945129792d4"},"cell_type":"code","source":["%%time\n","# Linear Classifier on Character Level TF IDF Vectors\n","accuracy4 = train_model(LogisticRegression(solver = 'lbfgs', max_iter = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n","print('LR, CharLevel Vectors: %.4f\\n' % accuracy4)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["LR, CharLevel Vectors: 0.8485\n","\n","CPU times: user 1.25 s, sys: 65 ms, total: 1.32 s\n","Wall time: 449 ms\n"],"name":"stdout"}]},{"metadata":{"id":"ZFK_LWTcLtED","colab_type":"code","colab":{}},"cell_type":"code","source":["results.loc['Logistic Regression'] = {\n","    'Count Vectors': accuracy1,\n","    'WordLevel TF-IDF': accuracy2,\n","    'N-Gram Vectors': accuracy3,\n","    'CharLevel Vectors': accuracy4}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q1wYto68LtEE","colab_type":"text"},"cell_type":"markdown","source":["### Support Vector Machine"]},{"metadata":{"id":"yYGz8he5LtEE","colab_type":"code","colab":{},"outputId":"546b1ea8-27c9-45bb-fd91-da08244d6796"},"cell_type":"code","source":["%%time\n","# Support Vector Machine on Count Vectors\n","accuracy1 = train_model(LinearSVC(), X_train_count, y_train, X_test_count)\n","print('SVM, Count Vectors    : %.4f\\n' % accuracy1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["SVM, Count Vectors    : 0.8345\n","\n","CPU times: user 994 ms, sys: 22.2 ms, total: 1.02 s\n","Wall time: 694 ms\n"],"name":"stdout"}]},{"metadata":{"id":"wMt26K0cLtEG","colab_type":"code","colab":{},"outputId":"406cf62d-c09d-4f9b-c298-eabce0b69238"},"cell_type":"code","source":["%%time\n","# Support Vector Machine on Word Level TF IDF Vectors\n","accuracy2 = train_model(LinearSVC(), X_train_tfidf, y_train, X_test_tfidf)\n","print('SVM, WordLevel TF-IDF : %.4f\\n' % accuracy2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["SVM, WordLevel TF-IDF : 0.8610\n","\n","CPU times: user 88.9 ms, sys: 4.43 ms, total: 93.3 ms\n","Wall time: 92.5 ms\n"],"name":"stdout"}]},{"metadata":{"id":"eFt6Y1VvLtEI","colab_type":"code","colab":{},"outputId":"628299db-8bca-4698-c64e-5b291bd248e1"},"cell_type":"code","source":["%%time\n","# Support Vector Machine on Ngram Level TF IDF Vectors\n","accuracy3 = train_model(LinearSVC(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n","print('SVM, N-Gram Vectors   : %.4f\\n' % accuracy3)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["SVM, N-Gram Vectors   : 0.8210\n","\n","CPU times: user 63.9 ms, sys: 4.01 ms, total: 68 ms\n","Wall time: 66.3 ms\n"],"name":"stdout"}]},{"metadata":{"id":"iqhsS579LtEL","colab_type":"code","colab":{},"outputId":"7cc3d723-9858-43a8-dcdf-37fcbef9456e"},"cell_type":"code","source":["%%time\n","# Support Vector Machine on Character Level TF IDF Vectors\n","accuracy4 = train_model(LinearSVC(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n","print('SVM, CharLevel Vectors: %.4f\\n' % accuracy4)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["SVM, CharLevel Vectors: 0.8570\n","\n","CPU times: user 493 ms, sys: 32.1 ms, total: 525 ms\n","Wall time: 522 ms\n"],"name":"stdout"}]},{"metadata":{"id":"go0bcKeILtEN","colab_type":"code","colab":{}},"cell_type":"code","source":["results.loc['Support Vector Machine'] = {\n","    'Count Vectors': accuracy1,\n","    'WordLevel TF-IDF': accuracy2,\n","    'N-Gram Vectors': accuracy3,\n","    'CharLevel Vectors': accuracy4}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gLGxWK0yLtEO","colab_type":"text"},"cell_type":"markdown","source":["### Bagging Models"]},{"metadata":{"id":"HR8aOytWLtEO","colab_type":"code","colab":{},"outputId":"961d5bca-c1fe-46be-ba8c-2f6325d85901"},"cell_type":"code","source":["%%time\n","# Bagging (Random Forest) on Count Vectors\n","accuracy1 = train_model(RandomForestClassifier(n_estimators = 100), X_train_count, y_train, X_test_count)\n","print('RF, Count Vectors    : %.4f\\n' % accuracy1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["RF, Count Vectors    : 0.8205\n","\n","CPU times: user 13.8 s, sys: 71 ms, total: 13.9 s\n","Wall time: 13.9 s\n"],"name":"stdout"}]},{"metadata":{"id":"zXcTCTEGLtET","colab_type":"code","colab":{},"outputId":"e4336045-c508-4372-e102-35c559f9a195"},"cell_type":"code","source":["%%time\n","# Bagging (Random Forest) on Word Level TF IDF Vectors\n","accuracy2 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf, y_train, X_test_tfidf)\n","print('RF, WordLevel TF-IDF : %.4f\\n' % accuracy2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["RF, WordLevel TF-IDF : 0.8260\n","\n","CPU times: user 5.57 s, sys: 44.3 ms, total: 5.61 s\n","Wall time: 5.64 s\n"],"name":"stdout"}]},{"metadata":{"id":"EnvT8qvSLtEW","colab_type":"code","colab":{},"outputId":"0c3a8fa0-7ee7-40ad-dc0e-85a8c3995733"},"cell_type":"code","source":["%%time\n","# Bagging (Random Forest) on Ngram Level TF IDF Vectors\n","accuracy3 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n","print('RF, N-Gram Vectors   : %.4f\\n' % accuracy3)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["RF, N-Gram Vectors   : 0.7870\n","\n","CPU times: user 5.58 s, sys: 53.9 ms, total: 5.63 s\n","Wall time: 5.66 s\n"],"name":"stdout"}]},{"metadata":{"id":"8jt-dTVELtEX","colab_type":"code","colab":{},"outputId":"dc1113c9-b3f2-4d2e-f4d7-a5bcc98a020c"},"cell_type":"code","source":["%%time\n","# Bagging (Random Forest) on Character Level TF IDF Vectors\n","accuracy4 = train_model(RandomForestClassifier(n_estimators = 100), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n","print('RF, CharLevel Vectors: %.4f\\n' % accuracy4)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["RF, CharLevel Vectors: 0.7885\n","\n","CPU times: user 22.4 s, sys: 237 ms, total: 22.6 s\n","Wall time: 21.3 s\n"],"name":"stdout"}]},{"metadata":{"id":"fVKeCH_VLtEZ","colab_type":"code","colab":{}},"cell_type":"code","source":["results.loc['Random Forest'] = {\n","    'Count Vectors': accuracy1,\n","    'WordLevel TF-IDF': accuracy2,\n","    'N-Gram Vectors': accuracy3,\n","    'CharLevel Vectors': accuracy4}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oyVz4Q6ILtEa","colab_type":"text"},"cell_type":"markdown","source":["### Boosting Models"]},{"metadata":{"id":"8wGvHTg-LtEb","colab_type":"code","colab":{},"outputId":"dcc31f90-b0f8-4f5d-c835-25be80638e70"},"cell_type":"code","source":["%%time\n","# Gradient Boosting on Count Vectors\n","accuracy1 = train_model(GradientBoostingClassifier(), X_train_count, y_train, X_test_count)\n","print('GB, Count Vectors    : %.4f\\n' % accuracy1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["GB, Count Vectors    : 0.7990\n","\n","CPU times: user 33.5 s, sys: 96.3 ms, total: 33.6 s\n","Wall time: 33.7 s\n"],"name":"stdout"}]},{"metadata":{"id":"HJNwQf57LtEd","colab_type":"code","colab":{},"outputId":"686abbc5-8745-4dca-c2e2-b665b9d19901"},"cell_type":"code","source":["%%time\n","# Gradient Boosting on Word Level TF IDF Vectors\n","accuracy2 = train_model(GradientBoostingClassifier(), X_train_tfidf, y_train, X_test_tfidf)\n","print('GB, WordLevel TF-IDF : %.4f\\n' % accuracy2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["GB, WordLevel TF-IDF : 0.7955\n","\n","CPU times: user 10.7 s, sys: 27.6 ms, total: 10.7 s\n","Wall time: 10.8 s\n"],"name":"stdout"}]},{"metadata":{"id":"iyPqLMgkLtEe","colab_type":"code","colab":{},"outputId":"b7c87e04-f724-4ccf-e02e-28beb7e40654"},"cell_type":"code","source":["%%time\n","# Gradient Boosting on Ngram Level TF IDF Vectors\n","accuracy3 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram, y_train, X_test_tfidf_ngram)\n","print('GB, N-Gram Vectors   : %.4f\\n' % accuracy3)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["GB, N-Gram Vectors   : 0.7350\n","\n","CPU times: user 6.38 s, sys: 15.5 ms, total: 6.4 s\n","Wall time: 6.41 s\n"],"name":"stdout"}]},{"metadata":{"id":"1KYdyatTLtEg","colab_type":"code","colab":{},"outputId":"aa4f303e-76ff-47e6-a04f-7e311e1848ad"},"cell_type":"code","source":["%%time\n","# Gradient Boosting on Character Level TF IDF Vectors\n","accuracy4 = train_model(GradientBoostingClassifier(), X_train_tfidf_ngram_chars, y_train, X_test_tfidf_ngram_chars)\n","print('GB, CharLevel Vectors: %.4f\\n' % accuracy4)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["GB, CharLevel Vectors: 0.8020\n","\n","CPU times: user 2min 37s, sys: 2.63 s, total: 2min 39s\n","Wall time: 1min 42s\n"],"name":"stdout"}]},{"metadata":{"scrolled":true,"id":"AC0hWO59LtEj","colab_type":"code","colab":{}},"cell_type":"code","source":["results.loc['Gradient Boosting'] = {\n","    'Count Vectors': accuracy1,\n","    'WordLevel TF-IDF': accuracy2,\n","    'N-Gram Vectors': accuracy3,\n","    'CharLevel Vectors': accuracy4}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b9fY4J7XLtEk","colab_type":"code","colab":{},"outputId":"ea4a8ddf-5b37-4cb9-da79-1920e1967b03"},"cell_type":"code","source":["results"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Count Vectors</th>\n","      <th>WordLevel TF-IDF</th>\n","      <th>N-Gram Vectors</th>\n","      <th>CharLevel Vectors</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Na√Øve Bayes</th>\n","      <td>0.8540</td>\n","      <td>0.8600</td>\n","      <td>0.840</td>\n","      <td>0.8180</td>\n","    </tr>\n","    <tr>\n","      <th>Logistic Regression</th>\n","      <td>0.8520</td>\n","      <td>0.8730</td>\n","      <td>0.836</td>\n","      <td>0.8485</td>\n","    </tr>\n","    <tr>\n","      <th>Support Vector Machine</th>\n","      <td>0.8345</td>\n","      <td>0.8610</td>\n","      <td>0.821</td>\n","      <td>0.8570</td>\n","    </tr>\n","    <tr>\n","      <th>Random Forest</th>\n","      <td>0.8205</td>\n","      <td>0.8260</td>\n","      <td>0.787</td>\n","      <td>0.7885</td>\n","    </tr>\n","    <tr>\n","      <th>Gradient Boosting</th>\n","      <td>0.7990</td>\n","      <td>0.7955</td>\n","      <td>0.735</td>\n","      <td>0.8020</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                        Count Vectors  WordLevel TF-IDF  N-Gram Vectors  \\\n","Na√Øve Bayes                    0.8540            0.8600           0.840   \n","Logistic Regression            0.8520            0.8730           0.836   \n","Support Vector Machine         0.8345            0.8610           0.821   \n","Random Forest                  0.8205            0.8260           0.787   \n","Gradient Boosting              0.7990            0.7955           0.735   \n","\n","                        CharLevel Vectors  \n","Na√Øve Bayes                        0.8180  \n","Logistic Regression                0.8485  \n","Support Vector Machine             0.8570  \n","Random Forest                      0.7885  \n","Gradient Boosting                  0.8020  "]},"metadata":{"tags":[]},"execution_count":44}]},{"metadata":{"id":"IS5Tc4z9FoYy","colab_type":"text"},"cell_type":"markdown","source":[">"]},{"metadata":{"id":"mxI2We9OFpfs","colab_type":"text"},"cell_type":"markdown","source":[">"]},{"metadata":{"id":"81DoNxN1FqGN","colab_type":"text"},"cell_type":"markdown","source":[">"]},{"metadata":{"id":"RERADKgNFq9T","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","> > > > > > > > > ¬© 2019 Data Science Institute of Australia\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n"]}]}